#!/usr/bin/env bun

/**
 * Run a single test case by ID and generate report
 *
 * Automatically starts eval environment servers, runs test, then cleans up.
 *
 * Usage: bun run evaluation/run-single-test.ts conv-01
 */

import { execSync, spawn } from 'node:child_process'
import { readFile, writeFile } from 'node:fs/promises'
import { join } from 'node:path'
import { generateMarkdownReport, generateReport } from './services/report-generator'
import { runTestCase } from './services/test-runner'
import type { TestCase } from './types'

const TEST_CASES_DIR = join(import.meta.dir, 'test-cases')
const FRONTEND_PORT = process.env.EVAL_FRONTEND_PORT || '3000'
const API_PORT = process.env.EVAL_API_PORT || '7070'
const FRONTEND_URL = `http://localhost:${FRONTEND_PORT}`
const API_URL = `http://localhost:${API_PORT}/api`

/**
 * Wait for servers to be ready by checking health endpoints
 */
async function waitForServers(): Promise<void> {
  let frontendReady = false
  let apiReady = false
  const startTime = Date.now()
  const TIMEOUT_MS = 30000 // 30 seconds

  return new Promise((resolve, reject) => {
    const checkInterval = setInterval(async () => {
      // Check frontend
      if (!frontendReady) {
        try {
          const response = await fetch(FRONTEND_URL, { signal: AbortSignal.timeout(2000) })
          if (response.ok) {
            frontendReady = true
            console.log('‚úÖ Frontend server ready')
          }
        } catch {
          // Not ready yet
        }
      }

      // Check API
      if (!apiReady) {
        try {
          const response = await fetch(`${API_URL}/health`, { signal: AbortSignal.timeout(2000) })
          if (response.ok) {
            apiReady = true
            console.log('‚úÖ API server ready')
          }
        } catch {
          // Not ready yet
        }
      }

      if (frontendReady && apiReady) {
        clearInterval(checkInterval)
        console.log('')
        resolve()
      }

      // Timeout check
      if (Date.now() - startTime > TIMEOUT_MS) {
        clearInterval(checkInterval)
        reject(new Error('Timeout waiting for servers to start'))
      }
    }, 1000)
  })
}

async function loadSingleTestCase(testId: string): Promise<TestCase | null> {
  // Try conversational first (files are named conversational-01.json, policy-01.json, etc.)
  try {
    const conversationalPath = join(
      TEST_CASES_DIR,
      'conversational',
      `conversational-${testId.replace('conv-', '')}.json`
    )
    const content = await readFile(conversationalPath, 'utf-8')
    return { ...JSON.parse(content), type: 'conversational' as const }
  } catch {
    // Try policy (files are named policy-01.json, etc.)
    try {
      const policyPath = join(
        TEST_CASES_DIR,
        'policy',
        `policy-${testId.replace('policy-', '')}.json`
      )
      const content = await readFile(policyPath, 'utf-8')
      return { ...JSON.parse(content), type: 'policy' as const }
    } catch {
      return null
    }
  }
}

async function main() {
  const testId = process.argv[2]

  if (!testId) {
    console.error('Usage: bun run evaluation/run-single-test.ts <test-id>')
    console.error('Example: bun run evaluation/run-single-test.ts conv-01')
    process.exit(1)
  }

  // Set environment variables
  process.env.EVALUATION_FRONTEND_URL = FRONTEND_URL
  process.env.EVALUATION_API_URL = API_URL

  console.log('üöÄ Starting evaluation environment...')
  console.log(`üì± Frontend: ${FRONTEND_URL}`)
  console.log(`üîå API: ${API_URL}`)
  console.log('')

  // Start eval environment servers
  const evalEnv = spawn('bun', ['run', join(import.meta.dir, 'start-eval-env.ts')], {
    stdio: 'pipe',
    shell: true,
    env: {
      ...process.env,
      FRONTEND_PORT,
      API_PORT,
    },
  })

  // Cleanup function
  const cleanup = (exitCode = 0) => {
    console.log('\nüõë Shutting down servers...')
    evalEnv.kill('SIGTERM')

    // Kill any remaining bun processes using kill-eval-servers.sh
    try {
      const killScript = join(import.meta.dir, 'kill-eval-servers.sh')
      execSync(`bash ${killScript}`, { stdio: 'inherit' })
    } catch (error) {
      console.error('‚ö†Ô∏è  Error running kill-eval-servers.sh:', error)
    }

    // Force kill after 2 seconds if not terminated
    setTimeout(() => {
      try {
        evalEnv.kill('SIGKILL')
      } catch {
        // Process already dead
      }
      process.exit(exitCode)
    }, 2000)
  }

  // Handle interrupts
  process.on('SIGINT', () => cleanup(130))
  process.on('SIGTERM', () => cleanup(143))

  try {
    // Wait for servers to be ready
    await waitForServers()

    console.log(`‚ñ∂Ô∏è  Running test: ${testId}`)
    console.log('')

    console.log(`üîç Loading test case: ${testId}`)
    const testCase = await loadSingleTestCase(testId)

    if (!testCase) {
      console.error(`‚ùå Test case "${testId}" not found`)
      cleanup(1)
      return
    }

    console.log(`üìã Test: ${testCase.name}`)
    console.log(
      `üìù Input: ${testCase.input || JSON.stringify(testCase.policyInput).substring(0, 100)}...\n`
    )

    console.log('‚ñ∂Ô∏è  Running test...\n')
    const result = await runTestCase(testCase)

    if (result.passed) {
      console.log('‚úÖ Test passed\n')
    } else {
      console.log(`‚ùå Test failed: ${result.error}\n`)
    }

    console.log('üìä Extraction Result:')
    if (result.actualResponse && typeof result.actualResponse === 'object') {
      const response = result.actualResponse as { profile?: unknown; extraction?: unknown }
      console.log(
        JSON.stringify(response.profile || response.extraction || result.actualResponse, null, 2)
      )
    }

    console.log('\nüìà Metrics:')
    if (result.metrics) {
      console.log(JSON.stringify(result.metrics, null, 2))
    }

    // Generate report for single test case
    console.log('\nüìù Generating report...')
    const report = await generateReport([result])
    const markdown = await generateMarkdownReport(report)

    // Write report files
    const resultDir = join(import.meta.dir, 'result')
    const jsonPath = join(resultDir, 'single-test-report.json')
    const mdPath = join(resultDir, 'single-test-report.md')

    await writeFile(jsonPath, JSON.stringify(report, null, 2))
    await writeFile(mdPath, markdown)

    console.log('\n‚úÖ Report written to:')
    console.log(`   - ${jsonPath}`)
    console.log(`   - ${mdPath}`)

    cleanup(0)
  } catch (error) {
    console.error('‚ùå Error:', error)
    cleanup(1)
  }
}

if (import.meta.main) {
  main().catch((error) => {
    console.error('‚ùå Error:', error)
    process.exit(1)
  })
}
