# Story 1.5: Conversational Extractor (LLM Agent for Field Extraction)

## Status
Ready to Implement

## Story
**As a** backend system,  
**I want** an LLM-powered agent that extracts structured shopper data from natural language dialogue,  
**so that** brokers can have flexible conversations without following rigid forms.

## Acceptance Criteria

1. POST `/api/intake` endpoint accepts broker message and conversation history
2. **Hybrid extraction approach**: First parse for key-value syntax (`kids:3`, `k:3`, `deps:4`, `car:garage`) using deterministic regex, then optionally use LLM for natural language extraction
3. Key-value syntax parser recognizes predefined field aliases (e.g., `k`/`kids`, `d`/`deps`, `v`/`vehicles`, `c`/`car`) and extracts structured data
4. LLM provider integration with structured output (JSON Schema) for natural language field extraction (optional/fallback when key-value not detected) - **CRITICAL**: Only use models that support JSON structured output. **Development**: Use Gemini 2.5 Flash-Lite (free tier) via Google GenAI SDK. **Provider interface pattern**: Abstract LLM calls behind interface for easy provider swapping (Gemini, OpenAI, etc.). **SDK usage**: Always use official SDKs (`@google/genai` for Gemini, `openai` for OpenAI). **Schema enforcement**: Use `zod-to-json-schema` for Gemini, `openai/helpers/zod` for OpenAI - avoid custom code, string together existing libraries.
5. Returns JSON response with extracted fields, confidence scores, and extraction method (key-value vs LLM)
6. Handles ambiguous natural language inputs gracefully - extracts what's clear, marks uncertain fields with low confidence
7. Token usage logged for LLM calls only (key-value extraction is free/deterministic)
8. Timeout configuration externalized to environment variable (default: 10 seconds for LLM)
9. Zod schema validation for extracted fields before returning to frontend
10. Decision trace logged: inputs, extraction method (key-value/LLM), extracted fields, confidence scores, reasoning
11. **Implementation note**: Framework should support iterative addition of field-specific shortcuts and aliases as broker workflows are discovered

## Tasks / Subtasks

- [ ] **Task 1: Create Conversational Extractor Service** (AC: 1, 2, 4, 9)
  - [ ] Create `apps/api/src/services/conversational-extractor.ts` service file
  - [ ] Implement `extractFields()` function that accepts `message: string` and `conversationHistory?: string[]`
  - [ ] Accept `llmProvider: LLMProvider` as constructor parameter (dependency injection for provider swapping)
  - [ ] Implement hybrid extraction: first try key-value parser, then LLM if needed
  - [ ] Return `ExtractionResult` type with fields, confidence scores, extraction method, missing fields
  - [ ] Use Zod schema validation for extracted fields (UserProfile schema from `@repo/shared`)
  - [ ] Handle extraction errors gracefully (return partial results with low confidence for uncertain fields)

- [ ] **Task 2: Integrate Key-Value Parser** (AC: 2, 3)
  - [ ] Reuse key-value parser from Story 1.4 (`apps/web/src/lib/key-value-parser.ts`) or create backend version
  - [ ] Create `apps/api/src/utils/key-value-parser.ts` utility (backend version)
  - [ ] Implement field alias recognition: `k`/`kids`, `d`/`deps`, `v`/`vehicles`, `c`/`car`, etc.
  - [ ] Parse key-value syntax: `kids:3`, `k:3`, `deps:4`, `car:garage`
  - [ ] Return structured data with extraction method `'key-value'` and confidence `1.0`
  - [ ] Case-insensitive key matching
  - [ ] Validate keys against UserProfile schema fields

- [ ] **Task 3: Implement LLM Provider Interface and Gemini Integration** (AC: 4, 6, 7, 8)
  - [ ] Create `apps/api/src/services/llm-provider.ts` interface defining `extractWithStructuredOutput()` method
  - [ ] Create `apps/api/src/services/gemini-provider.ts` implementing LLM provider interface
  - [ ] Install dependencies: `@google/genai` and `zod-to-json-schema` packages
  - [ ] Initialize Google GenAI client at server startup (reuse across requests) using `GoogleGenAI` from `@google/genai`
  - [ ] **Development model**: Use Gemini 2.5 Flash-Lite (`gemini-2.5-flash-lite`) - free tier, no API key required for development
  - [ ] **Schema flow**: Import `userProfileSchema` from `@repo/shared`, convert to JSON Schema using `zodToJsonSchema(userProfileSchema)` from `zod-to-json-schema` library
  - [ ] **LLM call with schema**: Implement `extractWithLLM()` using Gemini SDK: `ai.models.generateContent()` with:
    - `contents`: Conversation payload (message + conversation history as prompt)
    - `config.responseMimeType`: `"application/json"`
    - `config.responseJsonSchema`: Converted JSON Schema from Zod schema (sent with every LLM call to enforce response structure)
  - [ ] **Schema conversion**: Use `zod-to-json-schema` library to convert Zod schema to JSON Schema (no custom code) - this conversion happens on each LLM call
  - [ ] Configure timeout via environment variable `LLM_TIMEOUT_MS` (default: 10000ms)
  - [ ] Log token usage for every LLM call: extract token counts from Gemini response metadata
  - [ ] Handle Gemini API errors (timeout, rate limit, invalid response) with graceful degradation
  - [ ] Return extraction result with extraction method `'llm'` and confidence scores (0.0-1.0)
  - [ ] **Interface pattern**: Design provider interface to support future OpenAI provider implementation (easy swapping)

- [ ] **Task 4: Create POST /api/intake Endpoint** (AC: 1, 5)
  - [ ] Create `apps/api/src/routes/intake.ts` route file
  - [ ] Implement POST `/api/intake` handler using Hono
  - [ ] Accept request body: `{ message: string, conversationHistory?: string[] }`
  - [ ] Use Zod validator middleware (`@hono/zod-validator`) for request validation
  - [ ] Call Conversational Extractor service
  - [ ] Return `IntakeResult` response (will be extended in future stories with routing, discounts, pitch)
  - [ ] For MVP: Return `IntakeResult` with `profile` (extracted UserProfile), `missingFields` (array), `route` (stub), `opportunities` (empty array), `prefill` (stub), `pitch` (empty string), `complianceValidated` (true), `trace` (DecisionTrace)
  - [ ] Export route for registration in main app

- [ ] **Task 5: Implement Decision Trace Logging** (AC: 10)
  - [ ] Create `apps/api/src/utils/decision-trace.ts` utility
  - [ ] Implement `createDecisionTrace()` function
  - [ ] Log trace to compliance log file (`logs/compliance.log`)
  - [ ] Include: timestamp, flow type (`'conversational'`), inputs (message, conversationHistory), extraction (method, fields, confidence), llmCalls (if LLM used: model, tokens), outputs (extracted profile)
  - [ ] Use JSON format for compliance log entries
  - [ ] Ensure trace includes all required fields from DecisionTrace schema (`@repo/shared`)

- [ ] **Task 6: Add Error Handling** (AC: 6)
  - [ ] Handle extraction failures gracefully (return partial results with missing fields flagged)
  - [ ] Handle LLM provider API errors (timeout, rate limit) with user-friendly error messages
  - [ ] Use global error handler middleware for consistent error responses
  - [ ] Return `EXTRACTION_FAILED` error code if extraction completely fails
  - [ ] Log errors to program log (`logs/program.log`) with full context

- [ ] **Task 7: Create UserProfile Schema** (AC: 9)
  - [ ] Create `packages/shared/src/schemas/user-profile.ts` Zod schema
  - [ ] Define all fields from architecture docs: `state`, `productLine`, `age`, `householdSize`, `vehicles`, `ownsHome`, `cleanRecord3Yr`, `currentCarrier`, `currentPremium`, `existingPolicies` (array)
  - [ ] Mark most fields as optional (progressive disclosure pattern)
  - [ ] Export TypeScript type via `z.infer<typeof userProfileSchema>`
  - [ ] Export schema as `userProfileSchema` for use in Conversational Extractor validation and LLM structured outputs
  - [ ] **Schema usage**: This Zod schema will be converted to JSON Schema and sent with every LLM call to enforce response structure (see Task 3)

- [ ] **Task 8: Create IntakeResult Schema** (AC: 5)
  - [ ] Create `packages/shared/src/schemas/intake-result.ts` Zod schema
  - [ ] Define IntakeResult structure: `profile` (UserProfile), `missingFields` (string[]), `route` (RouteDecision stub), `opportunities` (Opportunity[]), `prefill` (PrefillPacket stub), `pitch` (string), `complianceValidated` (boolean), `trace` (DecisionTrace)
  - [ ] For MVP: Route, PrefillPacket, and Opportunity can be stub types (will be implemented in future stories)
  - [ ] Export TypeScript type via `z.infer<typeof intakeResultSchema>`
  - [ ] Export schema for API response validation

- [ ] **Task 9: Add Unit Tests** (AC: 2, 3, 4, 6)
  - [ ] Create `apps/api/src/services/__tests__/conversational-extractor.test.ts`
  - [ ] Test key-value parser: valid key-value pairs, field aliases, case-insensitive matching
  - [ ] Test LLM extraction: mock LLM provider interface, test structured output parsing, test error handling
  - [ ] Test hybrid extraction: key-value takes precedence, LLM fallback when no key-value detected
  - [ ] Test confidence scores: key-value = 1.0, LLM = variable (0.0-1.0)
  - [ ] Test missing fields detection: identify required fields not extracted
  - [ ] Test provider interface: verify Gemini provider implements interface correctly
  - [ ] Use Bun test framework with Jest-compatible API

- [ ] **Task 10: Add Integration Tests** (AC: 1, 5)
  - [ ] Create `apps/api/src/routes/__tests__/intake.test.ts`
  - [ ] Test POST `/api/intake` endpoint with Hono test utilities
  - [ ] Test request validation: invalid request body returns 400
  - [ ] Test successful extraction: key-value syntax returns correct profile
  - [ ] Test successful extraction: natural language returns extracted profile (mock Gemini provider)
  - [ ] Test error handling: LLM provider timeout returns graceful error
  - [ ] Test response format: IntakeResult matches schema
  - [ ] Use Hono test utilities (no server required)

- [ ] **Task 11: Update Main App to Register Route and Initialize Provider** (AC: 1)
  - [ ] Update `apps/api/src/index.ts` to register `/api/intake` route
  - [ ] Initialize Gemini provider at server startup: create `GoogleGenAI` instance, create `GeminiProvider` instance
  - [ ] Pass Gemini provider to Conversational Extractor service (dependency injection)
  - [ ] Import intake route handler
  - [ ] Mount route on app instance
  - [ ] Ensure route is accessible at `POST http://localhost:7070/api/intake`
  - [ ] Export `AppType` for Hono RPC client (frontend type safety)

- [ ] **Task 12: Add Environment Variable Configuration** (AC: 8)
  - [ ] Update `.env.example` with `GEMINI_API_KEY` (optional for free tier), `LLM_PROVIDER` (default: "gemini"), and `LLM_TIMEOUT_MS` (default: 10000)
  - [ ] Create typed config object in `apps/api/src/config/env.ts`
  - [ ] Access env vars through config object (never `process.env` directly)
  - [ ] **Gemini free tier**: `GEMINI_API_KEY` is optional - free tier works without API key for development
  - [ ] Use default timeout if `LLM_TIMEOUT_MS` not set
  - [ ] Provider selection: Use `LLM_PROVIDER` env var to select provider ("gemini" or future "openai")

## Dev Notes

### Previous Story Insights
- Story 1.4 completed: Frontend key-value parser exists (`apps/web/src/lib/key-value-parser.ts`), TanStack Query integration done, useIntake hook created, frontend ready for backend API
- Story 1.3 completed: Knowledge pack loader exists, RAG service interface created, health endpoint working, knowledge pack loaded at startup
- **Key-value parser pattern**: Frontend parser uses regex `/(\w+):(\w+|\d+)(?=\s|,|\.|$)/gi` to detect key-value pairs, supports field aliases, case-insensitive matching
- **Frontend integration**: Frontend expects `POST /api/intake` endpoint returning `IntakeResult` with `profile`, `missingFields`, `route`, `opportunities`, `prefill`, `pitch`, `complianceValidated`, `trace`
- **Optimistic UI updates**: Frontend updates sidebar immediately when key-value pair typed, reconciles with backend response

### Required Dependencies
- **Existing**: `openai` ^4.0, `zod` ^3.23 (already installed)
- **New packages to install**: `@google/genai` (Google GenAI SDK), `zod-to-json-schema` (Zod to JSON Schema converter)
- **Install command**: `cd apps/api && bun add @google/genai zod-to-json-schema`

### Data Models
- **UserProfile**: Schema defined in `packages/shared/src/schemas/user-profile.ts` with fields: `state` (required for routing), `productLine` (`'auto' | 'home' | 'renters' | 'umbrella'`), `age` (optional), `householdSize` (optional), `vehicles` (optional, required for auto), `ownsHome` (optional), `cleanRecord3Yr` (optional), `currentCarrier` (optional), `currentPremium` (optional), `existingPolicies` (optional array for bundle analysis) [Source: architecture/4-data-models.md#42-userprofile]
- **IntakeResult**: Schema includes `profile` (UserProfile), `missingFields` (string[]), `route` (RouteDecision), `opportunities` (Opportunity[]), `prefill` (PrefillPacket), `pitch` (string), `complianceValidated` (boolean), `trace` (DecisionTrace) [Source: architecture/4-data-models.md#45-intakeresult]
- **DecisionTrace**: Schema includes `timestamp`, `flow` (`'conversational' | 'policy'`), `inputs`, `extraction`, `routingDecision`, `discountCalculations`, `complianceCheck`, `llmCalls`, `rulesConsulted`, `outputs` [Source: architecture/4-data-models.md#48-decisiontrace]
- **Progressive disclosure pattern**: Most UserProfile fields optional to support conversational intake (extract what's mentioned, flag missing fields) [Source: architecture/4-data-models.md#42-userprofile]

### API Specifications
- **Intake Endpoint**: POST `/api/intake` accepts `{ message: string, conversationHistory?: array }` and returns `IntakeResult` [Source: architecture/5-api-specification.md#post-apiintake]
- **Request/Response Format**: JSON with snake_case keys (auto-transformed from camelCase TypeScript types) [Source: architecture/5-api-specification.md]
- **Hono RPC Client**: Frontend uses `api.api.intake.$post({ json: data })` then `.json()` to parse response [Source: architecture/5-api-specification.md#53-type-safe-api-client-hono-rpc]
- **Error Handling**: Standard error response format with `error.code`, `error.message`, `error.details` [Source: architecture/5-api-specification.md#54-error-handling]
- **Error Codes**: `EXTRACTION_FAILED` (400) for extraction failures, `LLM_API_ERROR` (503) for OpenAI API failures [Source: architecture/5-api-specification.md#54-error-handling]

### Component Specifications
- **Conversational Extractor Agent**: LLM-powered agent that extracts structured insurance shopper data from natural language input, uses a model with structured outputs (JSON Schema) support, supports progressive disclosure (extract what's mentioned, flag what's missing) [Source: architecture/6-components.md#61-conversational-extractor-agent-llm]
- **LLM Provider Interface Pattern**: Abstract LLM calls behind interface (`LLMProvider`) for easy provider swapping. Implementations: `GeminiProvider` (development, free tier), future `OpenAIProvider` (production). Dependency injection pattern allows swapping providers without changing extractor service code.
- **Model Selection**: **Development**: Use Gemini 2.5 Flash-Lite (`gemini-2.5-flash-lite`) - free tier, no API key required. **CRITICAL**: Only use models that support JSON structured output (not older JSON mode).
- **Hybrid extraction approach**: First parse for key-value syntax using deterministic regex, then optionally use LLM for natural language extraction [Source: architecture/8-core-workflows.md#81-conversational-intake-flow]
- **Key-value syntax parser**: Recognizes predefined field aliases (`k`/`kids`, `d`/`deps`, `v`/`vehicles`, `c`/`car`), extracts structured data, returns with extraction method `'key-value'` and confidence `1.0` [Source: architecture/8-core-workflows.md#81-conversational-intake-flow]
- **LLM extraction**: Uses Gemini SDK (`@google/genai`) with structured outputs (JSON Schema). **Schema flow**: UserProfile Zod schema (from Task 7) → converted to JSON Schema via `zod-to-json-schema` library → sent with conversation payload in `responseJsonSchema` field → LLM returns JSON matching schema structure. Zod schema enforcement ensures type safety, returns with extraction method `'llm'` and confidence scores (0.0-1.0) [Source: architecture/6-components.md#61-conversational-extractor-agent-llm]

### File Locations
- **Service files**: `apps/api/src/services/` directory [Source: architecture/11-backend-architecture.md#111-service-layer-architecture]
- **Route files**: `apps/api/src/routes/` directory [Source: architecture/11-backend-architecture.md#111-service-layer-architecture]
- **Utility files**: `apps/api/src/utils/` directory [Source: architecture/12-unified-project-structure.md]
- **Shared schemas**: `packages/shared/src/schemas/` directory [Source: architecture/4-data-models.md#41-core-data-models-overview]
- **Config files**: `apps/api/src/config/` directory [Source: architecture/17-coding-standards.md#171-critical-architectural-rules]

### Testing Requirements
- **Testing Framework**: Use Bun test (built-in, Jest-compatible API) [Source: architecture/16-testing-strategy.md#162-testing-tools]
- **Unit Tests**: Test key-value parser, LLM extraction, hybrid extraction, confidence scores, missing fields detection [Source: architecture/16-testing-strategy.md#163-testing-focus-areas]
- **Integration Tests**: Test POST `/api/intake` endpoint with Hono test utilities, request validation, error handling [Source: architecture/16-testing-strategy.md#162-testing-tools]
- **Test Focus Areas**: Deterministic engines (key-value parser), API routes (request validation, response structure, error handling) [Source: architecture/16-testing-strategy.md#163-testing-focus-areas]

### Technical Constraints
- **Backend Framework**: Hono 4.0 with TypeScript 5.6 [Source: architecture/3-tech-stack.md#31-technology-stack-table]
- **LLM Integration**: Google GenAI SDK (`@google/genai`) for Gemini, structured outputs (JSON Schema) [Source: docs/model-info/openai-gemini-structured-outputs-comparison.md]
- **Provider Interface Pattern**: Abstract LLM calls behind `LLMProvider` interface for easy provider swapping. Use dependency injection pattern.
- **Model Selection Constraint**: **CRITICAL** - Only use models that support JSON structured output (not older JSON mode). **Development**: Gemini 2.5 Flash-Lite (`gemini-2.5-flash-lite`) - free tier, no API key required.
- **SDK Usage**: Always use official SDKs - `@google/genai` for Gemini, `openai` for future OpenAI integration. Never write custom API clients.
- **Schema Conversion Libraries**: Use `zod-to-json-schema` for Gemini (converts Zod schema to JSON Schema), `openai/helpers/zod` for future OpenAI integration. Avoid custom schema conversion code.
- **Validation**: Zod ^3.23 for runtime type validation [Source: architecture/3-tech-stack.md#31-technology-stack-table]
- **Token usage logging**: Every LLM call must log token counts for cost tracking (required for PEAK6 evaluation) [Source: architecture/7-external-apis.md#71-openai-api]
- **Timeout configuration**: Externalized to environment variable `LLM_TIMEOUT_MS` (default: 10 seconds)
- **No streaming**: Synchronous responses simplify implementation for 5-day timeline [Source: architecture/7-external-apis.md#71-openai-api]
- **Environment Variables**: `GEMINI_API_KEY` (optional for free tier), `LLM_PROVIDER` (default: "gemini"), `LLM_TIMEOUT_MS` (default: 10000)

### Project Structure Notes
- **Service layer architecture**: Routes (thin layer) → Services (business logic) → Data layer (knowledge pack RAG) [Source: architecture/11-backend-architecture.md#111-service-layer-architecture]
- **Three-layer pattern**: Routes layer (HTTP concerns), Service layer (business logic), Data layer (knowledge pack RAG, in-memory Maps) [Source: architecture/11-backend-architecture.md#111-service-layer-architecture]
- **Middleware stack**: Error handler (global) → Logger middleware → Zod validator → Route handler [Source: architecture/11-backend-architecture.md#112-middleware-stack]
- **Dual logging pattern**: Program log (`logs/program.log`) for API requests, errors, LLM calls, token usage; Compliance log (`logs/compliance.log`) for DecisionTrace objects only [Source: architecture/11-backend-architecture.md#112-middleware-stack]

### Critical Implementation Notes
- **Provider Interface Pattern**: Create `LLMProvider` interface with `extractWithStructuredOutput()` method. Implement `GeminiProvider` class. Use dependency injection - pass provider to Conversational Extractor service constructor. This enables easy provider swapping without changing extractor service code.
- **SDK Usage**: Always use official SDKs - `@google/genai` for Gemini, `openai` for future OpenAI. Never write custom API clients or HTTP wrappers. String together existing libraries.
- **Schema Flow and LLM Integration**: 
  - **Step 1**: Create UserProfile Zod schema in Task 7 (`packages/shared/src/schemas/user-profile.ts`)
  - **Step 2**: In Gemini provider, import `userProfileSchema` from `@repo/shared`
  - **Step 3**: Convert Zod schema to JSON Schema using `zodToJsonSchema(userProfileSchema)` from `zod-to-json-schema` library
  - **Step 4**: Send JSON Schema with conversation payload in Gemini API call: `ai.models.generateContent({ contents: prompt, config: { responseMimeType: "application/json", responseJsonSchema: zodToJsonSchema(userProfileSchema) } })`
  - **Step 5**: LLM returns JSON matching the schema structure
  - **Step 6**: Validate response against Zod schema for type safety
  - **Key point**: Schema is sent with EVERY LLM call to enforce response structure - this is how structured outputs work
- **Schema Conversion Libraries**: Use `zod-to-json-schema` for Gemini (converts Zod schema to JSON Schema), `openai/helpers/zod` for future OpenAI integration. Avoid writing custom schema conversion code.
- **Model Selection**: **Development**: Use Gemini 2.5 Flash-Lite (`gemini-2.5-flash-lite`) - free tier, no API key required. **CRITICAL**: Only use models that support JSON structured output (not older JSON mode).
- **Hybrid extraction approach**: Parse key-value syntax FIRST (instant, free), then LLM only if natural language detected - hybrid extraction approach [Source: architecture/8-core-workflows.md#81-conversational-intake-flow]
- **Key-value parser**: Parse key-value syntax FIRST (instant, free), then LLM only if natural language detected - hybrid extraction approach [Source: architecture/8-core-workflows.md#81-conversational-intake-flow]
- **Structured outputs**: Use JSON Schema with Zod schema enforcement ensures type safety, prevents hallucinated field names. **How it works**: UserProfile Zod schema → converted to JSON Schema → sent with conversation payload in LLM API call → LLM returns JSON matching schema structure → validated against Zod schema [Source: architecture/6-components.md#61-conversational-extractor-agent-llm]
- **Token usage logging**: Every LLM call must log token counts for cost tracking (required for PEAK6 evaluation) [Source: architecture/7-external-apis.md#71-openai-api]
- **Decision trace logging**: Log to compliance log file (`logs/compliance.log`) with complete audit trail [Source: architecture/11-backend-architecture.md#112-middleware-stack]
- **Environment variables**: Access only through typed config objects, never `process.env` directly - catches missing env vars at startup (fail-fast) [Source: architecture/17-coding-standards.md#171-critical-architectural-rules]
- **GEMINI_API_KEY**: Optional for free tier - free tier works without API key for development
- **Error handling**: All API routes must use the global error handler middleware, never catch errors and return 200 with error in body [Source: architecture/17-coding-standards.md#171-critical-architectural-rules]
- **Progressive disclosure**: Extract what's mentioned, flag what's missing - enables progressive disclosure UX [Source: architecture/6-components.md#61-conversational-extractor-agent-llm]
- **Avoid Custom Code**: String together existing libraries (`@google/genai`, `zod-to-json-schema`, `zod`, `openai/helpers/zod`). Avoid writing custom API clients, schema converters, or HTTP wrappers.

## Testing

### Test File Location
- Unit tests: `apps/api/src/services/__tests__/`, `apps/api/src/utils/__tests__/`
- Integration tests: `apps/api/src/routes/__tests__/`

### Test Standards
- Use Bun test framework (Jest-compatible API: `describe`, `it`, `expect`) [Source: architecture/16-testing-strategy.md#162-testing-tools]
- Use Hono test utilities for API route testing (no server required) [Source: architecture/16-testing-strategy.md#162-testing-tools]
- Test key-value parser, LLM extraction, hybrid extraction, confidence scores, missing fields detection [Source: architecture/16-testing-strategy.md#163-testing-focus-areas]

### Testing Focus Areas
- Key-value syntax parsing (valid/invalid cases, field aliases, case-insensitive matching)
- LLM extraction (structured output parsing, error handling, timeout handling)
- Hybrid extraction (key-value takes precedence, LLM fallback when no key-value detected)
- Confidence scores (key-value = 1.0, LLM = variable 0.0-1.0)
- Missing fields detection (identify required fields not extracted)
- API endpoint (request validation, response structure, error handling)

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-11-10 | 1.0 | Initial story creation | Bob (Scrum Master) |
| 2025-11-10 | 1.1 | Added model selection constraint (JSON structured output only), free tokens information, OPENAI_API_KEY environment note | Bob (Scrum Master) |
| 2025-11-10 | 1.2 | Updated model list to only include confirmed structured output models (removed o1-mini, codex-mini-latest) | Bob (Scrum Master) |
| 2025-11-10 | 1.3 | Added cost-optimized model selection strategy (recommend cheapest models first: gpt-5-nano, gpt-4.1-nano, gpt-4o-mini) with pricing information | Bob (Scrum Master) |
| 2025-11-10 | 1.4 | Removed o3-mini, o4-mini, and codex-mini from model list (will never be used) | Bob (Scrum Master) |
| 2025-11-10 | 1.5 | Updated to use Gemini free tier for development, implemented provider interface pattern for easy provider swapping, emphasized SDK usage and existing libraries (avoid custom code) | Bob (Scrum Master) |

## Dev Agent Record

### Agent Model Used
_To be populated by Dev Agent_

### Debug Log References
_To be populated by Dev Agent_

### Completion Notes
_To be populated by Dev Agent_

### File List
_To be populated by Dev Agent_

## QA Results

_To be populated by QA Agent_

