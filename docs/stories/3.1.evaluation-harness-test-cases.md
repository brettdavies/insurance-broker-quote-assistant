# Story 3.1: Evaluation Harness & Test Cases

## Status

Ready for Review

## Story

**As a** developer,
**I want** an automated harness that runs 15 synthetic test cases and reports project metrics,
**so that** I can objectively validate system performance before demo.

## Acceptance Criteria

1. Create 15 synthetic test cases in `evaluation/test-cases/`: 10 conversational intake, 5 policy analysis
   - Note: Policy analysis test cases should validate both LLM identification (Story 2.2) and deterministic validation (Story 2.3)
2. Test cases cover all 3 carriers, 5 states, 4 product types with varied scenarios (complete data, missing fields, edge cases)
3. Command `bun run evaluate` executes all test cases and generates report
4. Report includes: routing accuracy %, intake completeness %, savings pitch clarity scores, compliance pass/fail
5. Per-carrier and per-state routing accuracy breakdown
6. Field completeness percentage for each required field across all tests
7. LLM token usage and cost breakdown (input/output tokens per test)
8. Sample decision traces with citations for audit
9. Report format: JSON + human-readable markdown summary
10. Evaluation harness achieves target metrics: ≥90% routing, ≥95% intake, ≥85% pitch, 100% compliance

## Tasks / Subtasks

- [x] **Task 1: Create Evaluation Directory Structure** (AC: 1)
  - [x] Create `evaluation/` directory at project root
  - [x] Create `evaluation/test-cases/` subdirectory
  - [x] Create `evaluation/harness.ts` file for automated runner
  - [x] Create `evaluation/report.md` template for markdown output
  - [x] Verify structure aligns with unified project structure [Source: architecture/12-unified-project-structure.md#evaluation-directory]

- [x] **Task 2: Create 10 Conversational Intake Test Cases** (AC: 1, 2)
  - [x] Create test case files in `evaluation/test-cases/conversational/` directory
  - [x] Test case 1: Complete data extraction (all fields present) - CA, Auto, GEICO
  - [x] Test case 2: Missing critical fields (state, product) - TX, Home, Progressive
  - [x] Test case 3: Partial data extraction (some optional fields missing) - FL, Renters, State Farm
  - [x] Test case 4: Edge case - ambiguous product mention - NY, Auto, GEICO
  - [x] Test case 5: Multi-product inquiry (auto + home bundle) - IL, Auto+Home, State Farm
  - [x] Test case 6: Age-based eligibility test - CA, Auto, Progressive (age 25)
  - [x] Test case 7: Clean record discount qualification - TX, Auto, GEICO
  - [x] Test case 8: State-specific requirements validation - FL, Home, State Farm
  - [x] Test case 9: Umbrella product routing - NY, Umbrella, GEICO
  - [x] Test case 10: Complex multi-field extraction - IL, Auto, Progressive (vehicles, drivers, mileage)
  - [x] Each test case includes: input message, expected profile fields, expected routing decision, expected opportunities
  - [x] Test cases cover all 3 carriers (GEICO, Progressive, State Farm) and all 5 states (CA, TX, FL, NY, IL)
  - [x] Test cases cover all 4 product types (auto, home, renters, umbrella)

- [x] **Task 3: Create 5 Policy Analysis Test Cases** (AC: 1, 2)
  - [x] Create test case files in `evaluation/test-cases/policy/` directory
  - [x] Test case 1: Policy with missing discounts (LLM identification + deterministic validation) - CA, Auto, GEICO
  - [x] Test case 2: Policy with bundle opportunity (single product → multi-product) - TX, Home, Progressive
  - [x] Test case 3: Policy with deductible optimization opportunity - FL, Auto, State Farm
  - [x] Test case 4: Policy with multiple opportunity types - NY, Auto+Home, GEICO
  - [x] Test case 5: Edge case - policy with no opportunities (already optimized) - IL, Auto, Progressive
  - [x] Each test case includes: policy text/data, expected PolicySummary, expected opportunities array, expected bundle options
  - [x] Test cases validate both LLM extraction (Story 2.2) and deterministic discount validation (Story 2.3)
  - [x] Test cases cover varied scenarios: complete data, missing fields, edge cases

- [x] **Task 4: Implement Evaluation Harness Runner** (AC: 3, 4, 5, 6, 7, 8, 9)
  - [x] Create `evaluation/harness.ts` using Bun test framework [Source: architecture/16-testing-strategy.md#bun-test]
  - [x] Implement test case loader: reads all test cases from `evaluation/test-cases/` directory
  - [x] Implement API client wrapper: calls `/api/intake` and `/api/policy/analyze` endpoints [Source: architecture/5-api-specification.md#endpoints]
  - [x] Implement metrics calculator:
    - [x] Routing accuracy: compare expected vs actual routing decisions (per carrier, per state)
    - [x] Intake completeness: calculate field completeness percentage per required field
    - [x] Savings pitch clarity: score pitches using rubric (≥85 points = pass) [Source: architecture/20-success-criteria-and-evaluation.md#savings-explanation-quality-rubric]
    - [x] Compliance pass rate: verify 100% compliance (all outputs pass compliance filter)
  - [x] Implement token usage tracker: capture input/output tokens from LLM calls [Source: architecture/11-backend-architecture.md#token-tracking]
  - [x] Implement decision trace collector: extract sample traces with citations from API responses
  - [x] Implement report generator:
    - [x] JSON report: structured data for programmatic analysis (`evaluation/report.json`)
    - [x] Markdown report: human-readable summary (`evaluation/report.md`)
  - [x] Add error handling: catch API errors, log failures, continue with remaining test cases

- [x] **Task 5: Add Root Package.json Script** (AC: 3)
  - [x] Add `"evaluate": "bun run evaluation/harness.ts"` script to root `package.json`
  - [x] Verify script executes harness and generates reports
  - [x] Document script usage in README (future story 3.5)

- [x] **Task 6: Implement Report Formatting** (AC: 4, 5, 6, 7, 8, 9)
  - [x] JSON report structure:
    - [x] Overall metrics: routing accuracy %, intake completeness %, pitch clarity avg, compliance pass %
    - [x] Per-carrier routing accuracy breakdown
    - [x] Per-state routing accuracy breakdown
    - [x] Field completeness percentages (per required field)
    - [x] LLM token usage: total input/output tokens, cost breakdown per test
    - [x] Sample decision traces array (3-5 representative traces with citations)
    - [x] Test case results: individual test pass/fail status, actual vs expected comparisons
  - [x] Markdown report structure:
    - [x] Executive summary: overall metrics vs target thresholds (≥90% routing, ≥95% intake, ≥85% pitch, 100% compliance)
    - [x] Per-carrier routing accuracy table
    - [x] Per-state routing accuracy table
    - [x] Field completeness table (showing % for each required field)
    - [x] LLM cost analysis: token usage summary, estimated cost per test case
    - [x] Sample decision traces: formatted citations showing knowledge pack references
    - [x] Test case details: expandable sections showing input, expected, actual, pass/fail
  - [x] Use consistent formatting: tables for metrics, code blocks for traces, clear section headers

- [x] **Task 7: Validate Target Metrics Achievement** (AC: 10)
  - [x] Run evaluation harness against all 15 test cases
  - [x] Verify routing accuracy ≥90% (per-carrier and per-state breakdowns meet threshold)
  - [x] Verify intake completeness ≥95% (field completeness percentages meet threshold)
  - [x] Verify savings pitch clarity ≥85% average score (using rubric from success criteria)
  - [x] Verify compliance pass rate = 100% (all outputs pass compliance filter)
  - [x] Document any metrics below thresholds in report with recommendations for improvement
  - [x] Add validation checks in harness: fail if metrics below thresholds (with clear error messages)

- [x] **Task 8: Unit Tests for Harness Components** (AC: All)
  - [x] Create `evaluation/__tests__/harness.test.ts` using Bun test framework [Source: architecture/16-testing-strategy.md#bun-test]
  - [x] Test metrics calculator: verify routing accuracy calculation logic
  - [x] Test intake completeness calculation: verify field completeness percentage logic
  - [x] Test pitch clarity scoring: verify rubric application (25 points per criterion)
  - [x] Test report generation: verify JSON and markdown output formats
  - [x] Test error handling: verify harness continues on API failures
  - [x] Use test data builders from `@repo/shared` for consistent test data [Source: architecture/16-testing-strategy.md#test-data-builders]

## Dev Notes

### Previous Story Insights

From Story 2.5 (Bundle Opportunity Detection):
- Integration test suite pattern: Comprehensive integration tests created in `apps/api/src/routes/__tests__/policy-bundle-integration.test.ts` with 13 test scenarios covering all acceptance criteria
- Test structure: Unit tests in `services/__tests__/`, integration tests in `routes/__tests__/` [Source: epic-3-evaluation-framework-production-deployment.md#implementation-notes]
- Decision trace integration: Decision traces logged with citations for audit trail - verified in integration tests
- Error handling: API errors gracefully handled without blocking analysis - verified in integration tests

### Data Models

**Test Case Structure:**
- Conversational intake test cases need: `input` (message string), `expectedProfile` (UserProfile), `expectedRoute` (RouteDecision), `expectedOpportunities` (Opportunity[])
- Policy analysis test cases need: `policyInput` (policy text or PolicySummary), `expectedPolicy` (PolicySummary), `expectedOpportunities` (Opportunity[]), `expectedBundleOptions` (BundleOption[])
- Test case metadata: `id`, `name`, `description`, `type` ('conversational' | 'policy'), `carrier`, `state`, `product`

**API Response Types:**
- IntakeResult: Contains `profile`, `missingFields`, `route`, `opportunities`, `pitch`, `complianceValidated`, `trace` [Source: architecture/4-data-models.md#intakeresult]
- PolicyAnalysisResult: Contains `currentPolicy`, `opportunities`, `bundleOptions`, `deductibleOptimizations`, `pitch`, `complianceValidated`, `trace` [Source: architecture/4-data-models.md#policyanalysisresult]
- DecisionTrace: Contains `timestamp`, `flow`, `inputs`, `extraction`, `routingDecision`, `discountCalculations`, `complianceCheck`, `llmCalls`, `rulesConsulted`, `outputs` [Source: architecture/4-data-models.md#decisiontrace]

**Metrics Calculation:**
- Routing accuracy: Compare `route.primaryCarrier` from API response vs expected carrier from test case
- Intake completeness: Calculate `(extractedFields / requiredFields) * 100` per field, then average across all fields
- Pitch clarity: Score using rubric (25 points each for: "because" rationale, discount percentages, dollar savings, citations) [Source: architecture/20-success-criteria-and-evaluation.md#savings-explanation-quality-rubric]
- Compliance: Verify `complianceValidated === true` for all test cases (100% required)

### API Specifications

**Endpoints to Test:**
- `POST /api/intake` - Request: `{ message: string, conversationHistory?: array }`, Response: `IntakeResult` [Source: architecture/5-api-specification.md#post-apiintake]
- `POST /api/policy/analyze` - Request: `{ policyText?: string, policyData?: PolicySummary, policyFile?: File }`, Response: `PolicyAnalysisResult` [Source: architecture/5-api-specification.md#post-apipolicyanalyze]

**API Client Pattern:**
- Use Hono RPC client pattern for type-safe API calls [Source: architecture/5-api-specification.md#type-safe-api-client-hono-rpc]
- For evaluation harness, can use direct `fetch()` calls since this is a test utility (not frontend component)
- Base URL: `http://localhost:7070/api` (configurable via environment variable)
- Error handling: API errors return structured error format with `error.code` and `error.message` [Source: architecture/5-api-specification.md#error-handling]

**Token Usage Tracking:**
- LLM token usage logged in DecisionTrace.llmCalls array: `{ agent: string, model: string, inputTokens: number, outputTokens: number }` [Source: architecture/11-backend-architecture.md#token-tracking]
- Extract token counts from `trace.llmCalls` in API responses
- Calculate cost using pricing from model-info docs (if available) or use standard OpenAI pricing

### File Locations

**Evaluation Directory Structure:**
- `evaluation/` - Root evaluation directory (at project root, parallel to `apps/`, `packages/`, `docs/`) [Source: architecture/12-unified-project-structure.md#evaluation-directory]
- `evaluation/test-cases/` - Test case files directory
- `evaluation/test-cases/conversational/` - Conversational intake test cases (10 files)
- `evaluation/test-cases/policy/` - Policy analysis test cases (5 files)
- `evaluation/result/` - Generated report files directory
- `evaluation/result/report.json` - Generated JSON report (gitignored)
- `evaluation/result/report.md` - Generated markdown report (gitignored)
- `evaluation/result/report_example.md` - Example report template (committed for reference)
- `evaluation/harness.ts` - Main evaluation harness runner
- `evaluation/__tests__/harness.test.ts` - Unit tests for harness components

**Test Case File Naming:**
- Conversational: `conversational-{number}.json` (e.g., `conversational-01.json`)
- Policy: `policy-{number}.json` (e.g., `policy-01.json`)
- Use kebab-case for file names [Source: architecture/17-coding-standards.md#naming-conventions]

**Script Location:**
- Root `package.json` - Add `evaluate` script to scripts section
- Script command: `bun run evaluation/harness.ts`

### Testing Requirements

**Test Framework:**
- Use Bun test framework (built-in, Jest-compatible API) [Source: architecture/16-testing-strategy.md#bun-test]
- Test file location: `evaluation/__tests__/harness.test.ts`
- Test structure: Use `describe`, `it`, `expect` from Bun test API

**Test Data:**
- Use test data builders from `@repo/shared` for consistent test data: `buildUserProfile()`, `buildRouteDecision()`, `buildCitation()` [Source: architecture/16-testing-strategy.md#test-data-builders]
- Create test case fixtures: Load test case JSON files in tests
- Mock API responses: Use `TestClient` pattern from backend test helpers if needed [Source: architecture/16-testing-strategy.md#backend-test-helpers]

**Test Coverage:**
- Unit tests for harness components: metrics calculator, report generator, error handling
- Integration tests: Run harness against mock API responses (use test targets system) [Source: architecture/16-testing-strategy.md#test-execution-modes]
- Test target: Use `'mock'` target for fast deterministic tests (default) [Source: architecture/16-testing-strategy.md#test-targets]

**Test Patterns:**
- Use parameterization with `test.each()` for repetitive test cases [Source: architecture/16-testing-strategy.md#parameterization]
- Use test data builders instead of manual object creation [Source: architecture/16-testing-strategy.md#test-data-builders]
- Follow DRY principles: eliminate duplication using shared utilities [Source: architecture/16-testing-strategy.md#dry-dont-repeat-yourself]

### Technical Constraints

**Success Criteria Targets:**
- Routing accuracy: ≥90% (per-carrier and per-state breakdowns) [Source: architecture/20-success-criteria-and-evaluation.md#peak6-requirements-mapping]
- Intake completeness: ≥95% (field completeness percentage) [Source: architecture/20-success-criteria-and-evaluation.md#peak6-requirements-mapping]
- Savings pitch clarity: ≥85% average score (using rubric: 25 points each for rationale, percentages, dollar savings, citations) [Source: architecture/20-success-criteria-and-evaluation.md#savings-explanation-quality-rubric]
- Compliance pass rate: 100% (all outputs must pass compliance filter) [Source: architecture/20-success-criteria-and-evaluation.md#peak6-requirements-mapping]

**Knowledge Pack Coverage:**
- Test cases must cover all 3 carriers: GEICO, Progressive, State Farm
- Test cases must cover all 5 states: CA, TX, FL, NY, IL
- Test cases must cover all 4 product types: auto, home, renters, umbrella
- Test cases must include varied scenarios: complete data, missing fields, edge cases

**Report Format Requirements:**
- JSON report: Machine-readable structured data for programmatic analysis
- Markdown report: Human-readable summary with tables, code blocks, clear sections
- Both reports generated in same run (not separate commands)

**Environment Configuration:**
- API base URL configurable via environment variable: `EVALUATION_API_URL` (default: `http://localhost:7070/api`)
- Harness should handle API server not running gracefully (clear error message)
- Token usage tracking requires DecisionTrace in API responses (verify trace is included)

### Project Structure Notes

**Evaluation Directory Placement:**
- `evaluation/` at project root (parallel to `apps/`, `packages/`, `docs/`) aligns with unified project structure [Source: architecture/12-unified-project-structure.md#evaluation-directory]
- This matches the architecture document specification for PEAK6 test cases and evaluation harness

**Script Integration:**
- Root `package.json` already has workspace scripts pattern: `"test": "bun test"`, `"dev": "concurrently ..."`
- Adding `"evaluate"` script follows same pattern: simple Bun command execution
- No need for workspace filtering since evaluation is at root level

**Gitignore Considerations:**
- Generated reports (`evaluation/report.json`, `evaluation/report.md`) should be gitignored (regenerated on each run)
- Test case files should be committed (they are source data, not generated)
- Consider adding `evaluation/report.*` to `.gitignore` if not already present

## Testing

### Test File Location
- `evaluation/__tests__/harness.test.ts` - Unit tests for harness components

### Test Standards
- Use Bun test framework (Jest-compatible API) [Source: architecture/16-testing-strategy.md#bun-test]
- Test structure: `describe` blocks for component grouping, `it` blocks for individual tests
- Use `expect` assertions from Bun test API

### Testing Frameworks and Patterns
- **Test Data Builders:** Use `buildUserProfile()`, `buildRouteDecision()`, `buildCitation()` from `@repo/shared` [Source: architecture/16-testing-strategy.md#test-data-builders]
- **Parameterization:** Use `test.each()` for repetitive test cases [Source: architecture/16-testing-strategy.md#parameterization]
- **Test Targets:** Use `'mock'` target for fast deterministic tests (default) [Source: architecture/16-testing-strategy.md#test-execution-modes]
- **DRY Principles:** Eliminate duplication using shared utilities [Source: architecture/16-testing-strategy.md#dry-dont-repeat-yourself]

### Specific Testing Requirements for This Story
- Unit tests for metrics calculator: verify routing accuracy, intake completeness, pitch clarity scoring logic
- Unit tests for report generator: verify JSON and markdown output formats
- Unit tests for error handling: verify harness continues on API failures
- Integration tests: Run harness against mock API responses (use test targets system)
- Test case validation: Verify test case JSON files are valid and loadable

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-11-XX | 1.0 | Initial story creation | Scrum Master |

## Dev Agent Record

### Agent Model Used

Claude Sonnet 4.5

### Debug Log References

N/A - No debug issues encountered during implementation

### Completion Notes List

- Created evaluation directory structure at project root with test-cases and result subdirectories
- Reports are generated in `evaluation/result/` directory (report.json and report.md are gitignored, report-template.md is committed for reference)
- Implemented 10 conversational intake test cases covering all carriers, states, and product types
- Implemented 5 policy analysis test cases with varied scenarios
- Built comprehensive evaluation harness with metrics calculation (routing accuracy, intake completeness, pitch clarity, compliance)
- Refactored harness.ts to follow DRY, STAR, and SOLID principles:
  - Separated concerns into service modules (test-case-loader, test-runner, metrics-calculator, report-generator, token-tracker, metrics-validator)
  - Implemented template-based markdown report generation using `report-template.md`
  - Extracted common formatting logic and eliminated code duplication
  - Each service has single responsibility and is independently testable
- Implemented token usage tracking and cost calculation using OpenAI pricing
- Created JSON and markdown report generators with template-based approach (similar to BMAD YAML templates)
- Added `bun run evaluate` script to root package.json
- Implemented validation checks for target metrics (≥90% routing, ≥95% intake, ≥85% pitch, 100% compliance)
- Created unit tests for harness components covering metrics calculator, report generation, and error handling
- All test cases follow kebab-case naming convention (conversational-01.json through conversational-10.json, policy-01.json through policy-05.json)

### File List

**Created Files:**
- `evaluation/harness.ts` - Main evaluation harness runner (orchestrates services)
- `evaluation/types.ts` - Shared type definitions
- `evaluation/services/test-case-loader.ts` - Loads test cases from filesystem
- `evaluation/services/test-runner.ts` - Executes test cases against API endpoints
- `evaluation/services/metrics-calculator.ts` - Calculates evaluation metrics
- `evaluation/services/report-generator.ts` - Generates JSON and markdown reports using template
- `evaluation/services/token-tracker.ts` - Extracts and calculates token usage/costs
- `evaluation/services/metrics-validator.ts` - Validates metrics against target thresholds
- `evaluation/result/report-template.md` - Markdown report template with {{variable}} placeholders (committed for reference)
- `evaluation/test-cases/conversational/conversational-01.json` through `conversational-10.json` - 10 conversational intake test cases
- `evaluation/test-cases/policy/policy-01.json` through `policy-05.json` - 5 policy analysis test cases
- `evaluation/__tests__/harness.test.ts` - Unit tests for harness components

**Modified Files:**
- `package.json` - Added `"evaluate": "bun run evaluation/harness.ts"` script
- `.gitignore` - Added `evaluation/result/report.json` and `evaluation/result/report.md` to ignore generated reports

## QA Results

_Results from QA Agent QA review of the completed story implementation_

